{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(1, 10)\n",
    "# 假设我们有一个样本，其类别索引为3（例如，在一个有4个类别的数据集中）\n",
    "class_index = torch.tensor([3])  # 一个包含类别索引的张量\n",
    "\n",
    "# 使用F.one_hot生成one-hot张量\n",
    "y = F.one_hot(class_index, num_classes=5)  # num_classes是类别的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size=5):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        # 定义第一个隐藏层\n",
    "        self.fc1 = nn.Linear(input_size, 7)\n",
    "        # 定义第二个隐藏层\n",
    "        self.fc2 = nn.Linear(7, 7)\n",
    "        # 定义第三个隐藏层\n",
    "        self.fc3 = nn.Linear(7, 7)\n",
    "        # 定义输出层\n",
    "        self.fc4 = nn.Linear(7, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用第一个隐藏层并使用ReLU激活函数\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 应用第二个隐藏层并使用ReLU激活函数\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 应用第三个隐藏层并使用ReLU激活函数\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # 应用输出层\n",
    "        x = self.fc4(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNet(input_size=dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1862, 0.2323, 0.1798, 0.1592, 0.2424]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(tensor)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = (torch.log(out)*y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0264, -0.0044,  0.0231,  0.0182,  0.0011, -0.0176,  0.0062,  0.0026,\n",
      "          0.0201, -0.0008],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [-0.0144,  0.0024, -0.0126, -0.0099, -0.0006,  0.0096, -0.0034, -0.0014,\n",
      "         -0.0109,  0.0004],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [ 0.0267, -0.0045,  0.0233,  0.0184,  0.0012, -0.0178,  0.0063,  0.0026,\n",
      "          0.0203, -0.0008]])\n",
      "tensor([-0.0149,  0.0000,  0.0000,  0.0081,  0.0000,  0.0000, -0.0150])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0649, -0.0000, -0.0000, -0.2223, -0.0000, -0.0000, -0.0983],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1360,  0.0000])\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0137, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0750, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0431,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0857, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0059, -0.0000]])\n",
      "tensor([-0.0467,  0.0000, -0.2547,  0.1465,  0.0000, -0.2911, -0.0200])\n",
      "tensor([[-0.0042, -0.0000, -0.0185, -0.0690, -0.0000, -0.0675, -0.0084],\n",
      "        [-0.0052, -0.0000, -0.0230, -0.0861, -0.0000, -0.0842, -0.0105],\n",
      "        [-0.0040, -0.0000, -0.0178, -0.0666, -0.0000, -0.0652, -0.0081],\n",
      "        [ 0.0188,  0.0000,  0.0834,  0.3115,  0.0000,  0.3048,  0.0380],\n",
      "        [-0.0054, -0.0000, -0.0240, -0.0898, -0.0000, -0.0879, -0.0110]])\n",
      "tensor([-0.1862, -0.2323, -0.1798,  0.8408, -0.2424])\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "\n",
    "# 遍历并打印参数\n",
    "for param in params:\n",
    "    print(param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
